{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diese Datei wurde fuer das erstellen aller .pkl Dateien im Ordner pickle benutzt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install impyute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from mypackage.MyMethods import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import pickle\n",
    "import secrets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import impyute as impy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#um komplette DataFrames auszugeben\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv(\"datasets/generated_data2.csv\")\n",
    "d1 = d1[[\"object_id\", \"time\", \"feature1\"]]\n",
    "d1 = d1.assign(feature1_nan=d1[[\"feature1\"]])\n",
    "\n",
    "pickle_dfs = []\n",
    "pickle_avg_distance = []\n",
    "pickle_best_rating = []\n",
    "\n",
    "secure_random = secrets.SystemRandom()\n",
    "z = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rota in range(1, 2):  #bestimmt wie viele gepickelte Dateien man haben möchte\n",
    "    for loops in range(1, 3): #bestimmt die Anzahl an DataFrames in einer gepickelten Datei\n",
    "        try:\n",
    "            d1 = d1[[\"object_id\", \"time\", \"feature1\", \"feature1_nan\"]]\n",
    "            ix = [(row, d1.columns.get_loc(\"feature1_nan\")) for row in d1[d1[\"feature1_nan\"].notna()].index]\n",
    "            sample_ix = secure_random.sample(ix, 158) # zweiter parameter bestimmt wie viele Datenpunkte pro Datensatz fehlen sollen\n",
    "\n",
    "            for row, col in sample_ix:\n",
    "                d1.iat[row, d1.columns.get_loc(\"feature1_nan\")] = np.nan\n",
    "\n",
    "            #Nicht Vorhandene Werte von Zeitreihen simulieren\n",
    "            sample_df = d1[[\"object_id\",\"time\",\"feature1_nan\"]].dropna()\n",
    "            sample_df = sample_df.rename(columns={\"feature1_nan\" : \"feature1\"})\n",
    "            sample_df[[\"time\", \"feature1\"]] = z.fit_transform(sample_df[[\"time\", \"feature1\"]])\n",
    "\n",
    "            rating_list = calc_best_rating(sample_df, 'object_id', 'time', 'feature1', 'assignments')\n",
    "\n",
    "            test_db1 = DBSCAN(eps = rating_list[1], min_samples = rating_list[2]).fit(sample_df[[\"time\", \"feature1\"]])\n",
    "            labsList = [\"Noise\"]\n",
    "            labsList = labsList + [\"Cluster \" + str(i) for i in range(1, len(set(test_db1.labels_)))]\n",
    "\n",
    "            sample_df[\"assignments\"] = test_db1.labels_\n",
    "            sample_df[[\"time\", \"feature1\"]] = z.inverse_transform(sample_df[[\"time\", \"feature1\"]])\n",
    "            sample_df[\"time\"] = sample_df[\"time\"].round()  #um rundungsfehler nach den scalen zu umgehen\n",
    "\n",
    "            #Erstellen aller DataFrames zu den dazugehoerigen Methoden. Interpolationsmethoden exclusive!\n",
    "            mfcm_df = most_frequent_cluster_member(sample_df, \"object_id\", \"time\", \"feature1\", \"assignments\")\n",
    "            mfcm_nearest_df = most_frequent_cluster_member_nearest(sample_df, \"object_id\", \"time\", \"feature1\", \"assignments\")\n",
    "            ppa_df = pre_and_post_clustering_analysis(sample_df, \"object_id\", \"time\", \"feature1\", \"assignments\")\n",
    "            new_method_mean_df = new_method_mean(sample_df, \"object_id\", \"time\", \"feature1\", \"assignments\")\n",
    "            new_method_median_df = new_method_median(sample_df, \"object_id\", \"time\", \"feature1\", \"assignments\")\n",
    "            new_method_mode_df = new_method_mode(sample_df, \"object_id\", \"time\", \"feature1\", \"assignments\")\n",
    "            mean_timestemp_df = mean_timestemp(sample_df, \"object_id\", \"time\", \"feature1\")\n",
    "            median_timestemp_df = median_timestemp(sample_df, \"object_id\", \"time\", \"feature1\")\n",
    "            mean_timeseries_df = mean_timeseries(sample_df, \"object_id\", \"time\", \"feature1\")\n",
    "            median_timeseries_df = median_timeseries(sample_df, \"object_id\", \"time\", \"feature1\")\n",
    "            knn_df = pd.DataFrame(impy.fast_knn(d1[[\"object_id\",\"time\",\"feature1_nan\"]].values, k=1), columns=[\"object_id\",\"time\",\"fast_knn\"])\n",
    "        except:\n",
    "            d1 = d1[[\"object_id\", \"time\", \"feature1\"]]\n",
    "            d1 = d1.assign(feature1_nan=d1[[\"feature1\"]])\n",
    "            continue\n",
    "\n",
    "        #um eine neue DataFrame an unser bestehendes DataFrames zu hängen\n",
    "        mfcm_df[\"time\"] = mfcm_df[\"time\"].astype(int)   #wegen float und int fehlern beim mergen\n",
    "        d1 = pd.merge(d1, mfcm_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.mfcm.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        ppa_df[\"time\"] = ppa_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, ppa_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.ppa.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        mfcm_nearest_df[\"time\"] = mfcm_nearest_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, mfcm_nearest_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.mfcm_nearest.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        new_method_mean_df[\"time\"] = new_method_mean_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, new_method_mean_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.new_method_mean.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        new_method_median_df[\"time\"] = new_method_median_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, new_method_median_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.new_method_median.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        new_method_mode_df[\"time\"] = new_method_mode_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, new_method_mode_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.new_method_mode.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        mean_timestemp_df[\"time\"] = mean_timestemp_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, mean_timestemp_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.mean_timestemp.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        median_timestemp_df[\"time\"] = median_timestemp_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, median_timestemp_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.median_timestemp.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        mean_timeseries_df[\"time\"] = mean_timeseries_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, mean_timeseries_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.mean_timeseries.fillna(d1.feature1_nan, inplace=True)\n",
    "\n",
    "        median_timeseries_df[\"time\"] = median_timeseries_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, median_timeseries_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.median_timeseries.fillna(d1.feature1_nan, inplace=True)\n",
    "        \n",
    "        #fast_knn\n",
    "        knn_df[\"time\"] = knn_df[\"time\"].astype(int)\n",
    "        d1 = pd.merge(d1, knn_df, how='left', on=[\"time\", \"object_id\"])\n",
    "        d1.fast_knn.fillna(d1.feature1_nan, inplace=True)\n",
    "        \n",
    "        #Fill-Mean und Fill-Median\n",
    "        d1 = d1.assign(FillMean=d1.feature1_nan.fillna(d1.feature1_nan.mean()))\n",
    "        d1 = d1.assign(FillMedian=d1.feature1_nan.fillna(d1.feature1_nan.median()))\n",
    "        \n",
    "        ids = list(set(getattr(d1,\"object_id\")))\n",
    "        interpolation_list = []\n",
    "        ##AKIMA INTERPOLATION\n",
    "        for object_id in ids:\n",
    "            time_series = d1[d1[\"object_id\"] == object_id]\n",
    "            time_series[[\"time\", \"feature1_nan\"]] = z.fit_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "            time_series = time_series.set_index('time')\n",
    "            try:    \n",
    "                our_result = time_series.assign(InterpolateAkima = time_series.feature1_nan.interpolate(method='akima'))\n",
    "                our_result = our_result.reset_index(\"time\")\n",
    "                our_result[[\"time\", \"InterpolateAkima\"]] = z.inverse_transform(our_result[[\"time\", \"InterpolateAkima\"]])\n",
    "                interpolation_list.append(our_result[[\"InterpolateAkima\", \"object_id\", \"time\"]])\n",
    "            except Exception as e:\n",
    "                time_series = time_series.reset_index(\"time\")\n",
    "                time_series[[\"time\", \"feature1_nan\"]] = z.inverse_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "                time_series = time_series.rename(columns={\"feature1_nan\" : \"InterpolateAkima\"})\n",
    "                interpolation_list.append(time_series[[\"InterpolateAkima\", \"object_id\", \"time\"]])\n",
    "                pass\n",
    "        d1 = pd.merge(d1, pd.concat(interpolation_list) , how='left', on=[\"time\", \"object_id\"])\n",
    "\n",
    "        \n",
    "        ##LINEAR INTERPOLATION\n",
    "        interpolation_list.clear()\n",
    "        for object_id in ids:\n",
    "            time_series = d1[d1[\"object_id\"] == object_id]\n",
    "            time_series[[\"time\", \"feature1_nan\"]] = z.fit_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "            time_series = time_series.set_index('time')\n",
    "            try:\n",
    "                our_result = time_series.assign(InterpolateLinear=time_series.feature1_nan.interpolate(method='linear'))\n",
    "                our_result = our_result.reset_index(\"time\")\n",
    "                our_result[[\"time\", \"InterpolateLinear\"]] = z.inverse_transform(our_result[[\"time\", \"InterpolateLinear\"]])\n",
    "                interpolation_list.append(our_result[[\"InterpolateLinear\",  \"object_id\", \"time\"]])\n",
    "            except Exception as e:\n",
    "                time_series = time_series.reset_index(\"time\")\n",
    "                time_series[[\"time\", \"feature1_nan\"]] = z.inverse_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "                time_series = time_series.rename(columns={\"feature1_nan\" : \"InterpolateLinear\"})\n",
    "                interpolation_list.append(time_series[[\"InterpolateLinear\", \"object_id\", \"time\"]])\n",
    "                pass\n",
    "        d1 = pd.merge(d1, pd.concat(interpolation_list) , how='left', on=[\"time\", \"object_id\"])\n",
    "\n",
    "        \n",
    "        ##QUADRATIC SPLINE INTERPOLATION\n",
    "        interpolation_list.clear()\n",
    "        for object_id in ids:\n",
    "            time_series = d1[d1[\"object_id\"] == object_id]\n",
    "            time_series[[\"time\", \"feature1_nan\"]] = z.fit_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "            time_series = time_series.set_index('time')\n",
    "            try:\n",
    "                our_result = time_series.assign(InterpolateQuadratic=time_series.feature1_nan.interpolate(method='quadratic'))\n",
    "                our_result = our_result.reset_index(\"time\")\n",
    "                our_result[[\"time\", \"InterpolateQuadratic\"]] = z.inverse_transform(our_result[[\"time\", \"InterpolateQuadratic\"]])\n",
    "                interpolation_list.append(our_result[[\"InterpolateQuadratic\", \"object_id\", \"time\"]])\n",
    "            except Exception as e:\n",
    "                time_series = time_series.reset_index(\"time\")\n",
    "                time_series[[\"time\", \"feature1_nan\"]] = z.inverse_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "                time_series = time_series.rename(columns={\"feature1_nan\" : \"InterpolateQuadratic\"})\n",
    "                interpolation_list.append(time_series[[\"InterpolateQuadratic\", \"object_id\", \"time\"]])\n",
    "                pass\n",
    "        d1 = pd.merge(d1, pd.concat(interpolation_list) , how='left', on=[\"time\", \"object_id\"])\n",
    "        \n",
    "        ##CUBIC SPLINE INTERPOLATION\n",
    "        interpolation_list.clear()\n",
    "        for object_id in ids:\n",
    "            time_series = d1[d1[\"object_id\"] == object_id]\n",
    "            time_series[[\"time\", \"feature1_nan\"]] = z.fit_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "            time_series = time_series.set_index('time')\n",
    "            try:\n",
    "                our_result = time_series.assign(InterpolateCubic= time_series.feature1_nan.interpolate(method='cubic'))\n",
    "                our_result = our_result.reset_index(\"time\")\n",
    "                our_result[[\"time\", \"InterpolateCubic\"]] = z.inverse_transform(our_result[[\"time\", \"InterpolateCubic\"]])\n",
    "                interpolation_list.append(our_result[[\"InterpolateCubic\", \"object_id\", \"time\"]])\n",
    "            except Exception as e:\n",
    "                time_series = time_series.reset_index(\"time\")\n",
    "                time_series[[\"time\", \"feature1_nan\"]] = z.inverse_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "                time_series = time_series.rename(columns={\"feature1_nan\" : \"InterpolateCubic\"})\n",
    "                interpolation_list.append(time_series[[\"InterpolateCubic\", \"object_id\", \"time\"]])\n",
    "                pass\n",
    "        d1 = pd.merge(d1, pd.concat(interpolation_list) , how='left', on=[\"time\", \"object_id\"])\n",
    "        \n",
    "\n",
    "        ## SPLINE INTERPOLATION GRAD 5\n",
    "        interpolation_list.clear()\n",
    "        for object_id in ids:\n",
    "            time_series = d1[d1[\"object_id\"] == object_id]\n",
    "            time_series[[\"time\", \"feature1_nan\"]] = z.fit_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "            time_series = time_series.set_index('time')\n",
    "            try:\n",
    "                our_result = time_series.assign(InterpolatePoly5=time_series.feature1_nan.interpolate(method='polynomial', order=5))\n",
    "                our_result = our_result.reset_index(\"time\")\n",
    "                our_result[[\"time\", \"InterpolatePoly5\"]] = z.inverse_transform(our_result[[\"time\", \"InterpolatePoly5\"]])\n",
    "                interpolation_list.append(our_result[[\"InterpolatePoly5\", \"object_id\", \"time\"]])\n",
    "            except Exception as e:\n",
    "                time_series = time_series.reset_index(\"time\")\n",
    "                time_series[[\"time\", \"feature1_nan\"]] = z.inverse_transform(time_series[[\"time\", \"feature1_nan\"]])\n",
    "                time_series = time_series.rename(columns={\"feature1_nan\" : \"InterpolatePoly5\"})\n",
    "                interpolation_list.append(time_series[[\"InterpolatePoly5\", \"object_id\", \"time\"]])\n",
    "                pass\n",
    "        d1 = pd.merge(d1, pd.concat(interpolation_list) , how='left', on=[\"time\", \"object_id\"])\n",
    "        \n",
    "        methods = list(d1)\n",
    "        delete_rows = [\"feature1\",\"feature1_nan\",\"time\",\"object_id\"]\n",
    "        for i in delete_rows:\n",
    "            methods.remove(i)\n",
    "\n",
    "        avg = [(method, avg_distance(d1.feature1, d1[method], d1[d1.feature1_nan.isna()].index)) for method in methods]\n",
    "        avg.sort(key=lambda x: x[1])\n",
    "\n",
    "        pickle_dfs.append(d1)\n",
    "        pickle_avg_distance.append(avg)\n",
    "        pickle_best_rating.append([rating_list[0], rating_list[1], rating_list[2]])\n",
    "        d1 = d1[[\"object_id\", \"time\", \"feature1\"]]\n",
    "        d1 = d1.assign(feature1_nan=d1[[\"feature1\"]])\n",
    "        print(loops)\n",
    "\n",
    "    pidfs = 'pickle/pickle_df_158_500.pkl'\n",
    "    with open(pidfs, 'wb') as pickle_file1:\n",
    "        pickle.dump(pickle_dfs, pickle_file1)\n",
    "\n",
    "    piavg = 'pickle/pickle_avg_158_500.pkl'\n",
    "    with open(piavg, 'wb') as pickle_file2:\n",
    "        pickle.dump(pickle_avg_distance, pickle_file2)\n",
    "\n",
    "    pira = 'pickle/pickle_rating_158_500.pkl'\n",
    "    with open(pira, 'wb') as pickle_file3:\n",
    "        pickle.dump(pickle_best_rating, pickle_file3)\n",
    "\n",
    "    d1 = d1[[\"object_id\", \"time\", \"feature1\"]]\n",
    "    d1 = d1.assign(feature1_nan=d1[[\"feature1\"]])\n",
    "    pickle_dfs.clear()\n",
    "    pickle_avg_distance.clear()\n",
    "    pickle_best_rating.clear()\n",
    "    print(rota)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
